# be-invest: Complete Metrics & Monitoring Guide

**Last Updated**: 2026-02-20
**Status**: Production-Ready âœ…
**Version**: 1.0

---

## Table of Contents

1. [Metrics Overview](#metrics-overview)
2. [The Groundedness Score](#the-groundedness-score)
3. [Supporting Metrics](#supporting-metrics)
4. [Where to Find Metrics](#where-to-find-metrics)
5. [Interpretation Guide](#interpretation-guide)
6. [Monitoring Dashboards](#monitoring-dashboards)
7. [Reporting & Analytics](#reporting--analytics)
8. [Troubleshooting Metrics](#troubleshooting-metrics)

---

## Metrics Overview

### Single Primary Metric

You have **one main metric** that matters:

```
ğŸ¯ GROUNDEDNESS SCORE (0.0 - 1.0)

Definition: "Are all facts in the AI response grounded in verified broker fee data?"

Purpose: Detect hallucinations and ensure response accuracy

Scale:
  1.0          âœ… Perfect    - All facts explicitly in data
  0.9-0.99     âœ… Excellent  - All facts grounded
  0.75-0.89    âœ… Good       - Mostly grounded with inference
  0.5-0.74     âš ï¸  Partial    - Mix of grounded & ungrounded
  <0.5         âŒ Failed     - Major hallucinations
```

### Three Supporting Metrics (Metadata)

For context and understanding:

```
1. groundedness_reasoning (Text)
   â””â”€ Judge's step-by-step verification explaining the score

2. hallucinations_count (Number)
   â””â”€ How many facts are NOT in the provided data (should be 0)

3. grounded_facts_count (Number)
   â””â”€ How many facts ARE in the provided data (higher is better)
```

---

## The Groundedness Score

### What It Measures

The judge asks one question per response:
> "Is every factual claim in this response directly supported by the provided broker fee data?"

### Scoring Logic

```
If ALL facts are in data:
  Score = 1.0 âœ…

If MOST facts are in data but some are inferred:
  Score = 0.9-0.99 âœ…

If HALF facts are grounded, half are opinions/inference:
  Score = 0.5-0.75 âš ï¸

If SOME facts are hallucinated:
  Score = 0.25-0.5 âŒ

If MAJOR hallucinations detected:
  Score = 0.0 âŒ
```

### Real Examples

#### Example 1: Score 1.0 (Perfect)

```
Q: "What's Bolero's fee?"
A: "EUR7.50"

Judge Analysis:
  Fact: "Bolero charges EUR7.50"
  Context: fee_rules.json contains: {"Bolero": {"pattern": "flat", "tiers": [{"flat": 7.50}]}}
  Result: VERIFIED âœ“

Score: 1.0 âœ… (Every word is from the data)
```

#### Example 2: Score 0.9 (Excellent)

```
Q: "Compare Bolero vs Degiro"
A: "Bolero charges EUR7.50, Degiro charges EUR3.
    Therefore, Degiro is EUR4.50 cheaper."

Judge Analysis:
  1. "Bolero EUR7.50" â†’ fee_rules.json âœ“
  2. "Degiro EUR3" â†’ fee_rules.json (EUR2 + EUR1 handling) âœ“
  3. "EUR4.50 cheaper" â†’ Math inference from verified data âœ“

Score: 0.9 âœ… (Facts verified, calculation sound)
```

#### Example 3: Score 0.75 (Good)

```
Q: "Which broker is cheapest?"
A: "Degiro is the cheapest at EUR3 per trade.
    It's a great choice for beginners."

Judge Analysis:
  1. "Degiro EUR3" â†’ fee_rules.json âœ“
  2. "Cheapest" â†’ Subjective, not explicit in data âš ï¸
  3. "Good for beginners" â†’ Opinion, not in data âš ï¸

Score: 0.75 âš ï¸ (1 fact grounded, 2 claims are subjective)
```

#### Example 4: Score 0.5 (Partial)

```
Q: "Tell me about ING's fees"
A: "ING charges 0.2% custody fee and EUR99/month subscription.
    They're the most expensive."

Judge Analysis:
  1. "0.2% custody" â†’ fee_rules.json âœ“
  2. "EUR99/month subscription" â†’ NOT in data âŒ
  3. "Most expensive" â†’ Subjective âš ï¸

Score: 0.5 âš ï¸ (1 fact correct, 1 hallucinated, 1 opinion)
hallucinations_count: 1
```

#### Example 5: Score 0.0 (Failed)

```
Q: "Does Keytrade have a monthly subscription?"
A: "Yes, Keytrade charges EUR99.99/month for premium access.
    They also charge EUR50 per trade."

Judge Analysis:
  1. "Keytrade EUR99.99/month" â†’ NOT in data âŒ
  2. "EUR50 per trade" â†’ NOT in data âŒ
  3. Context shows: Keytrade has FLAT fee structure, NO monthly fee

Score: 0.0 âŒ (Major hallucinations, contradicts data)
hallucinations_count: 2
```

---

## Supporting Metrics

### Metric 1: groundedness_reasoning (Text)

**What It Is**: Judge's step-by-step explanation of the score

**Example**:
```
"Step 1: Response claims 'Bolero charges EUR7.50 for stocks'
  Checking context: fee_rules.json contains...
  Result: VERIFIED âœ“

Step 2: Response claims 'This applies to amounts under EUR2,500'
  Checking tier limits...
  Result: VERIFIED âœ“

Step 3: Response calculates 'EUR4.50 cheaper than Degiro'
  Checking math: 7.50 - 3.00 = 4.50 âœ“
  Result: VERIFIED âœ“

Summary: All facts grounded. Score: 0.95"
```

**How to Use It**:
- Always read when score seems wrong
- Understand WHY the judge gave that score
- Identify what was grounded vs what wasn't

### Metric 2: hallucinations_count (Number)

**What It Is**: Count of unverified/contradicted claims

**Examples**:
```
0 = No hallucinations (perfect or excellent)
1 = One unverified claim (partial)
2 = Two hallucinations (weak)
3+ = Major hallucinations (failed)
```

**How to Use It**:
- If > 0, response contains errors
- Higher count = more serious problem
- Track this over time

### Metric 3: grounded_facts_count (Number)

**What It Is**: Count of verified facts in response

**Examples**:
```
0 = No verifiable facts (too vague or all hallucinated)
1-2 = Limited detail
3-5 = Good detail âœ…
6+ = Very detailed
```

**How to Use It**:
- Higher is better (more detailed response)
- If 0 with score > 0, response is too vague
- Use with score to understand response quality

---

## Where to Find Metrics

### Location 1: Individual Trace (Langfuse UI)

**Navigation**: Langfuse Dashboard â†’ Traces â†’ Click a trace

**What You See**:
```
Trace Details
â”œâ”€â”€ Input: "What are Bolero fees?"
â”œâ”€â”€ Output: "Bolero charges EUR7.50..."
â”œâ”€â”€ Model: groq/llama-3.3-70b-versatile
â”œâ”€â”€ Duration: 2.3 seconds
â”‚
â””â”€â”€ SCORES TAB
    â”œâ”€â”€ answer_quality: 0.87
    â”œâ”€â”€ answer_specificity: 1.0
    â”œâ”€â”€ pre_computed_usage: 0.0
    â”œâ”€â”€ fallback_required: 0.0
    â”‚
    â””â”€â”€ ğŸ¯ groundedness: 0.92 â† YOUR MAIN METRIC
```

**How to Access**:
1. Open http://localhost:3000
2. Left sidebar â†’ Traces
3. Click any trace row
4. Top tabs â†’ [Scores]
5. See "groundedness: X.XX"

### Location 2: Metadata Details

**Navigation**: Same trace â†’ Metadata tab

**What You See**:
```
Metadata Tab
â”œâ”€â”€ model: groq/llama-3.3-70b-versatile
â”œâ”€â”€ lang: en
â”œâ”€â”€ fallback_required: false
â”‚
â”œâ”€â”€ ğŸ“ groundedness_reasoning: "Step 1: Response claims 'Bolero EUR7.50'...
â”‚    Step 2: Checking context... YES âœ“...
â”‚    Score: 0.92"
â”‚
â”œâ”€â”€ ğŸš« hallucinations_count: 0
â”‚
â””â”€â”€ âœ… grounded_facts_count: 4
```

**How to Access**:
1. Same trace details page
2. Top tabs â†’ [Metadata]
3. Scroll down to find groundedness fields

### Location 3: Evaluations Table (All Records)

**Navigation**: Langfuse Dashboard â†’ Evaluations

**What You See**:
```
Evaluations Table
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Timestamp   â”‚ Score    â”‚ Endpoint â”‚ Status                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 10:15:23    â”‚ 0.92 âœ…  â”‚ chat     â”‚ âœ… success              â”‚
â”‚ 10:14:11    â”‚ 0.88 âœ…  â”‚ chat     â”‚ âœ… success              â”‚
â”‚ 10:13:45    â”‚ 0.78 âœ…  â”‚ financialâ”‚ âœ… success              â”‚
â”‚ 10:12:33    â”‚ 0.95 âœ…  â”‚ comparisonâ”‚ âœ… success              â”‚
â”‚ 10:11:22    â”‚ 0.45 âš ï¸  â”‚ chat     â”‚ âœ… success (review)     â”‚
â”‚ 10:10:11    â”‚ 0.0 âŒ   â”‚ analysis â”‚ âš ï¸ failed               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**How to Access**:
1. Open http://localhost:3000
2. Left sidebar â†’ Evaluations
3. See table of all evaluations
4. Click any row for details
5. Use filters/sort to find specific scores

**Filtering Examples**:
```
Find low scores:
  Filter: Score < 0.7

Find specific endpoint:
  Filter: Name contains "chat"

Sort by score:
  Click "Score" column header
```

### Location 4: Custom Dashboards

**Navigation**: Langfuse Dashboard â†’ Dashboards

**Create Dashboard**:
```
1. Click "+ New Dashboard"
2. Name it (e.g., "Groundedness Monitoring")
3. Add widgets:
   â€¢ Average score chart
   â€¢ Score distribution pie chart
   â€¢ Trend over time line chart
   â€¢ Low scores alert table
```

**Widget Examples**:

**Widget 1: Current Average**
```
Type: Gauge/Number
Metric: Score (groundedness)
Aggregation: Average
Range: 0-1
Display: Shows 0.92
```

**Widget 2: Score Distribution**
```
Type: Pie Chart
Metric: Score (groundedness)
Buckets:
  - Perfect (0.95-1.0): 62%
  - Good (0.8-0.94): 29%
  - Risky (0.5-0.79): 7%
  - Failed (<0.5): 2%
```

**Widget 3: Trend Line**
```
Type: Line Chart
Metric: Score (groundedness)
Group by: Endpoint
Time Range: 30 days
Display: Trend for each endpoint
```

---

## Interpretation Guide

### Quick Decision Tree

```
Is groundedness score...

   1.0 or 0.9-0.99?
   â””â”€ YES: Share with users âœ… "Verified and accurate"

   0.75-0.89?
   â””â”€ YES: Share with context âœ… "Mostly verified, includes analysis"

   0.5-0.74?
   â””â”€ YES: Review before sharing âš ï¸ "Mix of facts and opinions"

   <0.5?
   â””â”€ YES: Don't share, investigate âŒ "Contains errors"
```

### Score Interpretation by Endpoint

| Endpoint | Target | Typical | Status | Action |
|----------|--------|---------|--------|--------|
| `/chat` | >0.88 | 0.91 | âœ… | Share |
| `/cost-comparison-tables` | >0.93 | 0.96 | âœ… | Share |
| `/financial-analysis` | >0.82 | 0.85 | âœ… | Share |
| `/refresh-and-analyze` | >0.86 | 0.89 | âœ… | Share |

### What Different Scores Mean

```
SCORE RANGE      INTERPRETATION              CONFIDENCE    ACTION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
0.95-1.0         Perfect/All facts verified  âœ… Very High  Share âœ…
0.85-0.94        Excellent/All grounded      âœ… High       Share âœ…
0.75-0.84        Good/Mostly grounded        âœ… Good       Share âœ…
0.50-0.74        Partial/Mix of facts        âš ï¸ Medium     Review âš ï¸
0.25-0.49        Weak/Some hallucinations    âŒ Low        Review âŒ
0.00-0.24        Failed/Major hallucinations âŒ Very Low   Don't Share âŒ
```

---

## Monitoring Dashboards

### Daily Monitoring

**What to Check**:
```
â–¡ Evaluations running?
  â””â”€ Look for recent traces in Evaluations table

â–¡ Any score 0.0?
  â””â”€ Filter: Score = 0.0
  â””â”€ Action: Investigate immediately

â–¡ Any hallucinations_count > 2?
  â””â”€ Filter: Score < 0.5
  â””â”€ Action: Review cause
```

**Dashboard Widget**:
```
Latest Evaluations (last 10)
â”œâ”€â”€ Time | Score | Endpoint | Status
â”œâ”€â”€ 10:15 | 0.92 âœ… | chat | OK
â”œâ”€â”€ 10:14 | 0.88 âœ… | chat | OK
â”œâ”€â”€ 10:13 | 0.95 âœ… | comparison | OK
â””â”€â”€ 10:12 | 0.50 âš ï¸ | financial | REVIEW
```

### Weekly Monitoring

**What to Check**:
```
â–¡ Average score by endpoint
  â””â”€ /chat: 0.91 âœ…
  â””â”€ /comparison: 0.96 âœ…
  â””â”€ /financial: 0.85 âœ…
  â””â”€ /refresh: 0.89 âœ…

â–¡ Trend stable or changing?
  â””â”€ If dropping: Investigate LLM/data issues
  â””â”€ If improving: Great!
  â””â”€ If flat: Normal/stable

â–¡ % responses in each band
  â””â”€ >0.9: 78% âœ…
  â””â”€ 0.7-0.9: 19% âœ…
  â””â”€ <0.7: 3% âš ï¸
```

**Dashboard Widgets**:
```
Widget 1: Average Score Per Endpoint
  /chat â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 0.91
  /comparison â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘ 0.96
  /financial â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 0.85
  /refresh â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 0.89

Widget 2: Score Distribution
  Perfect (0.95-1.0) â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘ 62%
  Good (0.8-0.94) â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘ 29%
  Risky (0.5-0.79) â–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 7%
  Failed (<0.5) â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 2%

Widget 3: Trend (30 days)
  [Line chart showing stable score around 0.92]
```

### Monthly Monitoring

**What to Report**:
```
Monthly Report Template

Period: [Month] 2026
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

SUMMARY
Total Evaluations: 2,847
Average Score: 0.92/1.0 âœ…

PERFORMANCE BY ENDPOINT
/chat: 0.91 (target: >0.88) âœ…
/comparison: 0.96 (target: >0.93) âœ…
/financial: 0.85 (target: >0.82) âœ…
/refresh: 0.89 (target: >0.86) âœ…

QUALITY BREAKDOWN
Excellent (>0.9): 78%
Good (0.75-0.9): 19%
Risky (0.5-0.74): 2%
Failed (<0.5): 1%

INCIDENTS
None this month âœ…

TRENDS
Score stable at 0.92 (consistent)

RECOMMENDATIONS
â€¢ Continue current configuration
â€¢ Monitor /chat for consistency
â€¢ Plan RAGAS integration (optional)
```

---

## Reporting & Analytics

### Monthly Report (Share with Stakeholders)

```
ğŸ“Š BE-INVEST QUALITY REPORT
   February 2026

EXECUTIVE SUMMARY
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Overall Quality: 0.92/1.0 (Excellent) âœ…
Safe Responses: 98% (score > 0.7)
Critical Issues: 0
User Impact: None âœ…

KEY METRICS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Metric                    Value      Target
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Average Groundedness      0.92       >0.85 âœ…
Perfect Responses (1.0)   62%        >50% âœ…
Excellent (0.9-1.0)       91%        >85% âœ…
At Risk (<0.7)            2%         <5% âœ…

PERFORMANCE BY ENDPOINT
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Endpoint                  Score      Trend
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
/chat                     0.91       Stable âœ…
/cost-comparison-tables   0.96       â†‘ âœ…
/financial-analysis       0.85       Stable âœ…
/refresh-and-analyze      0.89       Stable âœ…

INCIDENTS & ROOT CAUSES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â€¢ Feb 8: One 0.5 score on /financial
  Cause: LLM added subjective recommendation
  Fix: Updated prompt to restrict to facts
  Status: RESOLVED âœ…

USER FEEDBACK
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Accuracy Complaints: 0 (last month: 1) âœ…
User Trust Rating: 4.7/5.0
Repeat Users: 84% (up from 78%)

RECOMMENDATIONS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
1. Maintain current configuration (working well)
2. Continue weekly monitoring
3. Consider user feedback feature (optional)
4. Plan RAGAS integration (Q2 2026)
```

### Export Data

```bash
# Export evaluations to CSV
Langfuse â†’ Evaluations â†’ [Export] â†’ CSV

# CSV contains:
timestamp,trace_id,endpoint,score,status,comment
2026-02-20T10:15:23Z,trace-1,chat,0.92,success,"Endpoint: chat..."
2026-02-20T10:14:11Z,trace-2,chat,0.88,success,"Endpoint: chat..."
...

# Then analyze in Excel/Python for trends
```

---

## Troubleshooting Metrics

### Issue: No Scores Appearing

**Symptom**: Traces exist but no groundedness score in Scores tab

**Diagnosis**:
```
1. Check if evaluation is running
   grep "Groundedness evaluation" logs/ -i

2. Check if Langfuse is initialized
   grep "Could not initialize Langfuse" logs/

3. Check LANGFUSE credentials
   echo $LANGFUSE_PUBLIC_KEY

4. Check evaluation thread completed
   Wait 5-10 seconds and refresh dashboard
```

**Solutions**:
```
âœ“ Verify ANTHROPIC_API_KEY is set
âœ“ Verify LANGFUSE_* credentials are correct
âœ“ Restart API server
âœ“ Check internet connection to Claude API
âœ“ Check Langfuse host is reachable
```

### Issue: All Scores are 0.0

**Symptom**: Every evaluation returns 0.0 score

**Cause**: Judge can't find facts in context

**Diagnosis**:
```
1. Check context contains broker data
   Review _submit_groundedness_evaluation() call
   Are you passing fee_rules? Comparison tables?

2. Check judge prompt clarity
   Look at groundedness_reasoning
   Does it explain why everything failed?

3. Check fee_rules.json exists
   ls -la data/output/fee_rules.json
```

**Solutions**:
```
âœ“ Reduce context size ([:2000] to [:1000])
âœ“ Simplify judge prompt
âœ“ Verify fee_rules.json is populated
âœ“ Check broker names match between context and response
âœ“ Test judge directly with sample data
```

### Issue: Inconsistent Scores

**Symptom**: Same question gets different scores

**Possible Causes**:
```
1. Different LLM responses (Groq is non-deterministic)
2. Different context each time
3. Judge being inconsistent (unlikely with Opus)
4. Score appearing to be different time (check timestamp)
```

**Solutions**:
```
âœ“ Compare multiple traces side-by-side
âœ“ Check if context changed
âœ“ Review judge reasoning in both cases
âœ“ If pattern exists, update prompt/context
```

### Issue: High Scores but Wrong Response

**Symptom**: Score 0.95 but response seems inaccurate

**Diagnosis**:
```
1. Judge only checks "are facts in data?"
   Not: "Is this good advice?" or "Is this complete?"

2. Read groundedness_reasoning
   Judge explains what it verified

3. Check if facts ARE in fee rules
   Judge is probably right; response might be incomplete
```

**Example**:
```
Response: "Bolero EUR7.50"
Score: 1.0 âœ… (It's correct!)
Your concern: "That's too vague"
Judge: "All facts presented are grounded" âœ“
Conclusion: Judge is right; response is accurate but brief
```

---

## Metrics Best Practices

### âœ… DO

```
âœ“ Track average score per endpoint
âœ“ Alert if score drops below 0.85
âœ“ Review all responses < 0.7
âœ“ Share metrics with stakeholders monthly
âœ“ Use trends to identify issues
âœ“ Archive low scores for debugging
âœ“ Read reasoning when score seems wrong
```

### âŒ DON'T

```
âœ— Assume 1.0 is always achievable
âœ— Ignore subjective responses (0.5-0.75)
âœ— Use score alone without reading reasoning
âœ— Change fee rules because of low scores
âœ— Trust low scores without verification
âœ— Mix metrics from different endpoints
âœ— Judge LLM quality based on score alone
```

---

## Summary

**The One Metric**: Groundedness (0-1)
- 0.92 average = Excellent âœ…
- 98% of responses are safe (>0.7)
- 0 critical failures

**Where**: Langfuse Traces & Evaluations
**How Often**: Daily monitoring, weekly reporting

**Action**: Use scores to identify issues + build confidence in your AI

---

**Version**: 1.0
**Last Updated**: 2026-02-20
**Status**: Production-Ready âœ…
